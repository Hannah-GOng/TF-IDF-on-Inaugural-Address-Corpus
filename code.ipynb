{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignment 2: Apply TF-IDF to the Inaugural Corpus \n",
    "Our second assignment will have us write code to \n",
    "1. Define or augment a set of stopwords for this problem\n",
    "2. Construct a document-by-term matrix (documents will be rows, terms will be columns), along with a vocabulary while controlling for stopwords\n",
    "3. Write functions to comput TF-IDF and apply those to the document-by-term matrix\n",
    "4. Find the closest historic inaugural address to the 2017 address by President Trump\n",
    "5. Learn to use the PCA transformation and plot the inaugural address along the first two principal components\n",
    "\n",
    "This assignment is to be done individually. Your code should be your own (with the exception of question 5, for which you're free to get some help from the web).\n",
    "\n",
    "**Due Date: 2020-09-20 5 pm ET**\n",
    "\n",
    "Please submit your completed assignment through GradeScope. You should submit a PDF of your notebook with all output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import math\n",
    "import pandas as pd\n",
    "\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.corpus import inaugural\n",
    "\n",
    "# Feel free to add your own libs as needed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = nltk.data.load('tokenizers/punkt/english.pickle')\n",
    "\n",
    "porter    = nltk.PorterStemmer()\n",
    "lancaster = nltk.LancasterStemmer()\n",
    "wnl       = nltk.WordNetLemmatizer()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['1789-Washington.txt',\n",
       " '1793-Washington.txt',\n",
       " '1797-Adams.txt',\n",
       " '1801-Jefferson.txt']"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.fileids()[:4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Fellow', '-', 'Citizens', 'of', 'the', 'Senate', ...]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "inaugural.words(inaugural.fileids()[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "stop_words = set(stopwords.words('english'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Stopwords\n",
    "\n",
    "**Update stopwords once you've had a chance to explore the text**\n",
    "\n",
    "Remove all punctuation and strange unicode characters, and anything else you think might be extraneous"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# here is where you should update the stop words\n",
    "stop_words.update(...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 2. Read each inaugural address into an Pandas DataFrame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.1 Create a vocabulary as a set of all unique stemmed terms in the corpus**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = set()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code for creating the vocab goes here\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2.2 Use your vocabulary now to read each inaugural address into a dataframe**\n",
    "\n",
    "Each row of the dataframe should represent a document (one of the addresses). It may be handy at this time to also track the size (length) of each document, since you'll need this later when computing TF.\n",
    "\n",
    "You should ignore the README file. Hint: use `inaugural.fileids()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      a     b     c     d\n",
      "0   2.0   3.0   4.0   NaN\n",
      "1  20.0  30.0  40.0  50.0\n"
     ]
    }
   ],
   "source": [
    "# Feel free to use your own approach but you can iteratively update a dataframe using dictionaries.\n",
    "# Here's an example\n",
    "df =pd.DataFrame()\n",
    "\n",
    "dict1 = {'a':2, 'b':3, 'c':4}\n",
    "dict2 = {'b':30, 'a':20, 'd':50, 'c':40}\n",
    "\n",
    "df = df.append(dict1, ignore_index=True)\n",
    "df = df.append(dict2, ignore_index=True)\n",
    "\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame()\n",
    "# your code for creating the dataframe goes here.\n",
    "# the resulting dataframe should be called \"df\"\n",
    "#\n",
    "# Each row of the dataframe should represent a document (inaugural address)\n",
    "# Each column of the dataframe should be a term from the vocab\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(inaugural.fileids())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 3. Compute TF-IDF for the document-term matrix ###"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1. Write a function to compute term frequency (TF) for each document**\n",
    "\n",
    "Please write your own code here, and resist the urge to rely on google for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# compute term frequency\n",
    "# inputs: wordvec is a series that contains, for a given doc, \n",
    "#                 the word counts for each term in the vocab\n",
    "#         doclen  is the length of the document\n",
    "# returns: a series with new term-frequencies (raw counts divided by doc length)\n",
    "def computetf(wordvec,doclen):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2 Write a function to comput inverse document frequency**\n",
    "\n",
    "Please write your own code here and resiste the urge to rely on google for your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math # you may need this for the log function\n",
    "\n",
    "# input:   document-by-term (row-by-column) dataframe\n",
    "# returns: dictionary of key-value pairs. Keys are terms in the vocab, values are IDF.\n",
    "\n",
    "def computeidf(df):\n",
    "    ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Create a new dataframe and populate it with the TF-IDF values for each document-term combination**\n",
    "\n",
    "The functions your write above should work with the below code snippet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.DataFrame()\n",
    "\n",
    "idfdict = computeidf(df)\n",
    "\n",
    "cols = df.columns\n",
    "for index, row in df.iterrows():\n",
    "    newrow = computetf(row,sizearr[index])\n",
    "    for c in cols:\n",
    "        newrow[c] = newrow[c]*idfdict[c]\n",
    "    newdf = newdf.append(newrow)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(58, 4439)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "newdf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 4. Using TF-IDF values, find and rank order the 3 closest inaugural addresses to Donald Trump's 2017 address, measured by cosine similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "# President Trump's address is 57 (0-indexed)\n",
    "# newdf.iloc[57,:].head(100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1 Create an array called dist that contains the cosine similarity distance between the 2017 inaugural address (called d1 below) and each of the inaugural addresses**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "d1 = newdf.iloc[57,:]\n",
    "dist = []\n",
    "for index,row in newdf.iterrows():\n",
    "    # your code goes here\n",
    "    ...\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dist"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2 Find the 3 closest associated inaugural address, when measured by cosign similarity. Which one is the closest?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3 What is your explanation/understanding of why these documents are \"close\" to the 2017? Please explain."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Your explanation goes here*\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### 5. Compute the first two principal components of the TF-IDF matrix, and plot each document along each of the PCA components\n",
    "For this question, feel free to use google, stackoverflow, etc to help you compute the PCA (it's pretty easy, just one or two lines). Don't worry too much about the theory for now - we're going to discuss the Principal Component Decomposition later in the semester."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "# your code to compute the PCA goes here\n",
    "# The result should be X, an array of 2-element arrays\n",
    "..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "58"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X[:, 0], X[:, 1], alpha=0.2)\n",
    "plt.axis('equal');\n",
    "for i in range(0,58):\n",
    "    plt.text(X[i,0],X[i,1],inaugural.fileids()[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
